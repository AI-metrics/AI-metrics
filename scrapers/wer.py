import os
import re
import string
import urllib2

from BeautifulSoup import BeautifulSoup
from dateutil import parser
import mistune

from data.acoustics import speech_recognition
from scales import error_rate # or error_percent?
from taxonomy import offline

# needs to be manually updated when more metrics are added
h2_name_map = {
        "LibriSpeech" : "librispeech",
        "WSJ" : "wsj",
        "Switchboard Hub5'00" : "swb_hub_500",
        "Fisher" : "fisher",
        "CHiME (noisy speech)" : "chime",
        "TIMIT" : "timit"
        }

def get_metrics(h2_name, first_row, file_output, scale = "error_percent", targets = (None, None), target_source = None):
    metric_names = []
    name = h2_name_map[h2_name]
    if name == "":
        raise SystemExit("Error! Need to add h2_name to h2_name_map to be able to parse!")
    for column, target in zip(first_row, targets):
        column = str(column.getText()).translate(None, string.punctuation)
        metric_name = str(name + "_" + "_".join(re.findall(r"[\w']+", column))).replace("test", "")
        metric_names.append(metric_name)
        if targets != (None, None):
            s = "{0} = speech_recognition.metric(name=\"{1} {2}\", scale={3}, target={4}, target_source=\"{5}\")\n".format(metric_name, name, column, scale, target, target_source)
        else:
            s = "{0} = speech_recognition.metric(name=\"{1} {2}\", scale={3})\n".format(metric_name, name, column, scale)
        file_output += s
    return (metric_names, file_output)

def add_measures(metric_names, row):
    def row_data(row):
        columns = row.findAll('td')
        l = len(columns)
        url = columns[l-3].find('a', href=True)['href']
        columns = map(lambda x: x.getText(), columns)
        date = str(parser.parse(columns[l-2]).date()).split("-")
        date = "date({0}, {1}, {2})".format(int(date[0]), int(date[1]), int(date[2]))
        r = {
                'name': columns[l-1].encode('ascii', 'ignore'),
                'date': date,
                'url': url,
                'values': map(lambda x: x.strip('%'), columns[:-3])
            }
        return r
    data = row_data(row)
    notes = data['name']
    targets = []
    if notes == "Humans":
        targets = data['values']
        return ([], targets, data['url'])
    table = []
    for metric_name, value in zip(metric_names, data['values']):
        if not value:
            continue
        s = "{0}.measure({1}, {2}, '{3}', '{4}')\n".format(metric_name, data['date'], value, notes, data['url'])
        table.append(s)
    return (table, targets, '')


def main():
    md = urllib2.urlopen('https://raw.githubusercontent.com/syhw/wer_are_we/master/README.md').read()
    bs = BeautifulSoup(mistune.markdown(md))
    wer_data_file = os.path.abspath(os.path.join(os.path.dirname(__file__),  "../data/wer.py"))
    file_output = "# The file was autogenerated by ../scrapers/wer.py\n\nfrom datetime import date\n\nfrom data.acoustics import speech_recognition, swb_hub_500\nfrom scales import *\n\n"
    wer_metrics = []
    for table, header in zip(bs.findAll('table'), bs.findAll('h3')):
        header = header.getText()
        rows = table.findAll('tr')
        metric_data = get_metrics(header, rows[0].findAll('th')[:-3], file_output)
        metric_names = metric_data[0]
        wer_metrics += metric_names
        table_data = []
        for row in rows:
            if row.findAll('td') == []:
                continue
            measure_data, targets, target_source = add_measures(metric_names, row)
            if not targets:
                table_data += measure_data
            elif not measure_data:
                metric_data = get_metrics(header, rows[0].findAll('th')[:-3], file_output, targets = targets, target_source = target_source)
        file_output = metric_data[1]
        file_output += "".join(sorted(table_data))
    file_output = file_output + "\n\nwer_metrics=[" + ", ".join(wer_metrics) + "]"

    with open(wer_data_file, 'wb') as f:
        f.write(file_output)

if not offline:
    main()
